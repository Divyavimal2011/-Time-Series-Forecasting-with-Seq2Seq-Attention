# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFwTrkjwjI0FR4jUTpkukquH_EtFCp3d
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # reduce TF logging
import warnings
warnings.filterwarnings('ignore')  # suppress user warnings for a clean output

import math, json, random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

# reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

OUTDIR = './outputs'
os.makedirs(OUTDIR, exist_ok=True)

# ------------------ 1) Generate compact dataset (~3000 obs) ------------------
N = 3000
index = pd.date_range(start='2020-01-01', periods=N, freq='H')

t = np.arange(N)
trend = 0.00004 * (t**2) - 0.008 * t
seasonal_daily = 6 * np.sin(2 * np.pi * t / 24)
seasonal_weekly = 2 * np.cos(2 * np.pi * t / (24*7) + 0.5)
low_freq = 1.5 * np.sin(2 * np.pi * t / 700 + 0.9)
noise_std = 0.35 + 0.25 * (np.abs(low_freq) + 0.2 * (t / N))
noise = np.random.randn(N) * noise_std
temperature = 20 + 5 * np.sin(2 * np.pi * t / (24*30)) + 1.0 * np.sin(2 * np.pi * t / 24)
promotion = (np.random.rand(N) < 0.01).astype(float)
series = 30 + trend + seasonal_daily + seasonal_weekly + low_freq + noise + 2.0 * promotion + 0.2 * temperature

df = pd.DataFrame({'y': series, 'temperature': temperature, 'promotion': promotion}, index=index)

# save generated dataset
df.to_csv(os.path.join(OUTDIR, 'generated_time_series_fast.csv'))

# ------------------ 2) Windowing ------------------
PAST = 168  # 7 days
HORIZON = 24
FEATURES = ['y', 'temperature', 'promotion']

values = df[FEATURES].values.astype(np.float32)
scaler = StandardScaler()
values_scaled = scaler.fit_transform(values)

X, Y = [], []
for i in range(PAST, len(values_scaled) - HORIZON + 1):
    X.append(values_scaled[i-PAST:i])
    Y.append(values_scaled[i:i+HORIZON, 0])
X = np.array(X)
Y = np.array(Y)

ns = X.shape[0]
train_end = int(ns * 0.70)
val_end = int(ns * 0.85)
X_train, Y_train = X[:train_end], Y[:train_end]
X_val, Y_val = X[train_end:val_end], Y[train_end:val_end]
X_test, Y_test = X[val_end:], Y[val_end:]

print('Prepared data shapes:', X_train.shape, Y_train.shape, X_val.shape, X_test.shape)

# ------------------ 3) Lightweight Seq2Seq + Attention (vectorized) ------------------
class SimpleBahdanauAttention(layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, values, queries):
        # values: (batch, Tenc, Denc)
        # queries: (batch, Tdec, Ddec)
        # compute score for each query against each value
        # values_proj -> (batch, 1, Tenc, units)
        v = tf.expand_dims(self.W1(values), axis=1)
        # queries_proj -> (batch, Tdec, 1, units)
        q = tf.expand_dims(self.W2(queries), axis=2)
        score = self.V(tf.nn.tanh(v + q))  # (batch, Tdec, Tenc, 1)
        score = tf.squeeze(score, -1)      # (batch, Tdec, Tenc)
        weights = tf.nn.softmax(score, axis=-1)
        context = tf.matmul(weights, values)  # (batch, Tdec, Denc)
        return context, weights


def build_fast_seq2seq(past, n_features, horizon, enc_units=48, dec_units=48):
    enc_in = layers.Input(shape=(past, n_features), name='enc_in')
    # avoid Masking layer to keep attention warnings away
    enc = layers.Bidirectional(layers.LSTM(enc_units, return_sequences=True), name='enc_bi')(enc_in)

    dec_in = layers.Input(shape=(horizon, 1), name='dec_in')
    dec = layers.LSTM(dec_units*2, return_sequences=True)(dec_in)

    att = SimpleBahdanauAttention(units=32)
    context, att_weights = att(enc, dec)

    concat = layers.Concatenate(axis=-1)([context, dec])
    x = layers.TimeDistributed(layers.Dense(64, activation='relu'))(concat)
    out = layers.TimeDistributed(layers.Dense(1))(x)
    out = layers.Reshape((horizon,))(out)

    model = Model([enc_in, dec_in], out)
    att_model = Model([enc_in, dec_in], att_weights)
    return model, att_model

seq_model, att_model = build_fast_seq2seq(PAST, X_train.shape[2], HORIZON, enc_units=48, dec_units=36)
seq_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')
seq_model.summary()

# prepare decoder teacher-forcing inputs (vectorized)
def make_decoder_inputs(Y, last_obs):
    n = Y.shape[0]
    dec = np.zeros((n, HORIZON, 1), dtype=np.float32)
    dec[:, 0, 0] = last_obs
    if HORIZON > 1:
        dec[:, 1:, 0] = Y[:, :-1]
    return dec

dec_train = make_decoder_inputs(Y_train, X_train[:, -1, 0])
dec_val = make_decoder_inputs(Y_val, X_val[:, -1, 0])
dec_test = make_decoder_inputs(Y_test, X_test[:, -1, 0])

# callbacks (save in TensorFlow SavedModel format to avoid HDF5 messages)
cp_path = os.path.join(OUTDIR, 'seq_fast_best.keras')
callbacks = [
    ModelCheckpoint(cp_path, monitor='val_loss', save_best_only=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1),
    EarlyStopping(monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)
]

history = seq_model.fit([X_train, dec_train], Y_train, validation_data=([X_val, dec_val], Y_val), epochs=15, batch_size=64, callbacks=callbacks)

# ------------------ 4) Baseline lightweight LSTM ------------------
def build_fast_lstm(past, n_features, horizon, units=64):
    inp = layers.Input(shape=(past, n_features))
    x = layers.LSTM(units)(inp)
    x = layers.Dropout(0.1)(x)
    out = layers.Dense(horizon)(x)
    model = Model(inp, out)
    model.compile(optimizer='adam', loss='mse')
    return model

lstm_model = build_fast_lstm(PAST, X_train.shape[2], HORIZON, units=64)
cp_lstm = os.path.join(OUTDIR, 'lstm_fast_best.keras')
cb_l = [ModelCheckpoint(cp_lstm, save_best_only=True, verbose=0)]
history_l = lstm_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=15, batch_size=64, callbacks=cb_l)

# ------------------ 5) Predictions & evaluation ------------------
def inv_target(scaled):
    mean = scaler.mean_[0]
    scale = scaler.scale_[0]
    return scaled * scale + mean

# autoregressive seq2seq prediction

def predict_seq_autoregressive(model, X_in):
    n = X_in.shape[0]
    preds = np.zeros((n, HORIZON), dtype=np.float32)
    dec = np.zeros((n, HORIZON, 1), dtype=np.float32)
    dec[:, 0, 0] = X_in[:, -1, 0]
    for t in range(HORIZON):
        out = model.predict([X_in, dec], verbose=0)
        preds[:, t] = out[:, t]
        if t + 1 < HORIZON:
            dec[:, t+1, 0] = out[:, t]
    return preds

seq_scaled = predict_seq_autoregressive(seq_model, X_test)
lstm_scaled = lstm_model.predict(X_test)

seq_preds = inv_target(seq_scaled)
lstm_preds = inv_target(lstm_scaled)
Y_test_raw = inv_target(Y_test)

# metrics
results = []
for name, pred in [('Seq2Seq+Attn', seq_preds), ('Baseline LSTM', lstm_preds)]:
    rm = math.sqrt(mean_squared_error(Y_test_raw.flatten(), pred.flatten()))
    ma = mean_absolute_error(Y_test_raw.flatten(), pred.flatten())
    # simplified MASE using seasonal m=24
    denom = np.mean(np.abs(np.diff(df['y'].values, n=24))) if len(df) > 24 else np.mean(np.abs(np.diff(df['y'].values)))
    mase_val = np.mean(np.abs(Y_test_raw - pred)) / (denom + 1e-9)
    results.append({'model': name, 'RMSE': float(rm), 'MAE': float(ma), 'MASE': float(mase_val)})

pd.DataFrame(results).to_csv(os.path.join(OUTDIR, 'performance_fast.csv'), index=False)
print('\nPerformance:')
print(pd.DataFrame(results))

# ------------------ 6) Plots & attention heatmap ------------------
plt.figure(figsize=(10,3))
plt.plot(df.index[-500:], df['y'].values[-500:])
plt.title('Last 500 points of generated series (fast)')
plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, 'series_fast_last500.png'))

# example forecast plot
for i in range(min(3, X_test.shape[0])):
    start = val_end + i
    times = df.index[PAST + start: PAST + start + HORIZON]
    plt.figure(figsize=(8,2.5))
    plt.plot(times, Y_test_raw[i], marker='o', label='True')
    plt.plot(times, seq_preds[i], marker='x', label='Seq2Seq')
    plt.plot(times, lstm_preds[i], marker='.', label='LSTM')
    plt.legend(); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, f'forecast_fast_{i}.png'))

# attention weights for first 3 test samples
try:
    att_w = att_model.predict([X_test[:3], dec_test[:3]])  # (samples, Tdec, Tenc)
    for i in range(att_w.shape[0]):
        plt.figure(figsize=(7,2.5))
        plt.imshow(att_w[i], aspect='auto')
        plt.colorbar(); plt.title(f'Attention weights sample {i}')
        plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, f'att_fast_{i}.png'))
except Exception:
    # if attention extraction fails, continue without stopping
    pass

# final report
with open(os.path.join(OUTDIR, 'report_fast.json'), 'w') as f:
    json.dump({'performance': results}, f, indent=2)

print('\nAll fast outputs saved to', OUTDIR)

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # reduce TF logging
import warnings
warnings.filterwarnings('ignore')  # suppress user warnings for a clean output

import math, json, random, io
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

# reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

OUTDIR = './outputs'
os.makedirs(OUTDIR, exist_ok=True)

# ------------------ 1) Generate dataset (>=5000 obs, still fast) ------------------
N = 5000  # increased to meet project minimum
index = pd.date_range(start='2020-01-01', periods=N, freq='H')

t = np.arange(N)
trend = 0.00002 * (t**2) - 0.006 * t
seasonal_daily = 6 * np.sin(2 * np.pi * t / 24)
seasonal_weekly = 2 * np.cos(2 * np.pi * t / (24*7) + 0.5)
low_freq = 1.5 * np.sin(2 * np.pi * t / 700 + 0.9)
noise_std = 0.35 + 0.25 * (np.abs(low_freq) + 0.2 * (t / N))
noise = np.random.randn(N) * noise_std
temperature = 20 + 5 * np.sin(2 * np.pi * t / (24*30)) + 1.0 * np.sin(2 * np.pi * t / 24)
promotion = (np.random.rand(N) < 0.01).astype(float)
series = 30 + trend + seasonal_daily + seasonal_weekly + low_freq + noise + 2.0 * promotion + 0.2 * temperature

df = pd.DataFrame({'y': series, 'temperature': temperature, 'promotion': promotion}, index=index)

df.to_csv(os.path.join(OUTDIR, 'generated_time_series.csv'))
# save basic summary statistics
pd.DataFrame(df.describe()).to_csv(os.path.join(OUTDIR, 'generated_summary_stats.csv'))

# ------------------ 2) Windowing ------------------
PAST = 168  # 7 days
HORIZON = 24
FEATURES = ['y', 'temperature', 'promotion']

values = df[FEATURES].values.astype(np.float32)
scaler = StandardScaler()
values_scaled = scaler.fit_transform(values)

X, Y = [], []
for i in range(PAST, len(values_scaled) - HORIZON + 1):
    X.append(values_scaled[i-PAST:i])
    Y.append(values_scaled[i:i+HORIZON, 0])
X = np.array(X)
Y = np.array(Y)

ns = X.shape[0]
train_end = int(ns * 0.70)
val_end = int(ns * 0.85)
X_train, Y_train = X[:train_end], Y[:train_end]
X_val, Y_val = X[train_end:val_end], Y[train_end:val_end]
X_test, Y_test = X[val_end:], Y[val_end:]

print('Prepared data shapes:', X_train.shape, Y_train.shape, X_val.shape, X_test.shape)

# ------------------ 3) Lightweight Seq2Seq + Attention (vectorized) ------------------
class SimpleBahdanauAttention(layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, values, queries):
        # values: (batch, Tenc, Denc)
        # queries: (batch, Tdec, Ddec)
        v = tf.expand_dims(self.W1(values), axis=1)   # (batch,1,Tenc,units)
        q = tf.expand_dims(self.W2(queries), axis=2)  # (batch,Tdec,1,units)
        score = self.V(tf.nn.tanh(v + q))             # (batch,Tdec,Tenc,1)
        score = tf.squeeze(score, -1)                # (batch,Tdec,Tenc)
        weights = tf.nn.softmax(score, axis=-1)
        context = tf.matmul(weights, values)          # (batch,Tdec,Denc)
        return context, weights


def build_fast_seq2seq(past, n_features, horizon, enc_units=48, dec_units=48):
    enc_in = layers.Input(shape=(past, n_features), name='enc_in')
    enc = layers.Bidirectional(layers.LSTM(enc_units, return_sequences=True), name='enc_bi')(enc_in)

    dec_in = layers.Input(shape=(horizon, 1), name='dec_in')
    dec = layers.LSTM(dec_units*2, return_sequences=True)(dec_in)

    att = SimpleBahdanauAttention(units=32)
    context, att_weights = att(enc, dec)

    concat = layers.Concatenate(axis=-1)([context, dec])
    x = layers.TimeDistributed(layers.Dense(64, activation='relu'))(concat)
    out = layers.TimeDistributed(layers.Dense(1))(x)
    out = layers.Reshape((horizon,))(out)

    model = Model([enc_in, dec_in], out)
    att_model = Model([enc_in, dec_in], att_weights)
    return model, att_model

seq_model, att_model = build_fast_seq2seq(PAST, X_train.shape[2], HORIZON, enc_units=48, dec_units=36)
seq_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')

# capture model summary
buf = io.StringIO()
seq_model.summary(print_fn=lambda x: buf.write(x + "\n"))
model_summary = buf.getvalue()
with open(os.path.join(OUTDIR, 'seq_model_summary.txt'), 'w') as f:
    f.write(model_summary)

# prepare decoder teacher-forcing inputs (vectorized)
def make_decoder_inputs(Y, last_obs):
    n = Y.shape[0]
    dec = np.zeros((n, HORIZON, 1), dtype=np.float32)
    dec[:, 0, 0] = last_obs
    if HORIZON > 1:
        dec[:, 1:, 0] = Y[:, :-1]
    return dec

dec_train = make_decoder_inputs(Y_train, X_train[:, -1, 0])
dec_val = make_decoder_inputs(Y_val, X_val[:, -1, 0])
dec_test = make_decoder_inputs(Y_test, X_test[:, -1, 0])

# callbacks
cp_path = os.path.join(OUTDIR, 'seq_fast_best.keras')
callbacks = [
    ModelCheckpoint(cp_path, monitor='val_loss', save_best_only=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1),
    EarlyStopping(monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)
]

history = seq_model.fit([X_train, dec_train], Y_train, validation_data=([X_val, dec_val], Y_val), epochs=15, batch_size=64, callbacks=callbacks)

# ------------------ 4) Baseline lightweight LSTM ------------------
def build_fast_lstm(past, n_features, horizon, units=64):
    inp = layers.Input(shape=(past, n_features))
    x = layers.LSTM(units)(inp)
    x = layers.Dropout(0.1)(x)
    out = layers.Dense(horizon)(x)
    model = Model(inp, out)
    model.compile(optimizer='adam', loss='mse')
    return model

lstm_model = build_fast_lstm(PAST, X_train.shape[2], HORIZON, units=64)
cp_lstm = os.path.join(OUTDIR, 'lstm_fast_best.keras')
cb_l = [ModelCheckpoint(cp_lstm, save_best_only=True, verbose=0)]
history_l = lstm_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=15, batch_size=64, callbacks=cb_l)

# ------------------ 5) Predictions & evaluation ------------------
def inv_target(scaled):
    mean = scaler.mean_[0]
    scale = scaler.scale_[0]
    return scaled * scale + mean

# autoregressive seq2seq prediction

def predict_seq_autoregressive(model, X_in):
    n = X_in.shape[0]
    preds = np.zeros((n, HORIZON), dtype=np.float32)
    dec = np.zeros((n, HORIZON, 1), dtype=np.float32)
    dec[:, 0, 0] = X_in[:, -1, 0]
    for t in range(HORIZON):
        out = model.predict([X_in, dec], verbose=0)
        preds[:, t] = out[:, t]
        if t + 1 < HORIZON:
            dec[:, t+1, 0] = out[:, t]
    return preds

seq_scaled = predict_seq_autoregressive(seq_model, X_test)
lstm_scaled = lstm_model.predict(X_test)

seq_preds = inv_target(seq_scaled)
lstm_preds = inv_target(lstm_scaled)
Y_test_raw = inv_target(Y_test)

# metrics
results = []
# compute seasonal naive denominator (m=24) from training series
m = 24
train_series = df['y'].values[:train_end+PAST]
if len(train_series) > m:
    denom = np.mean(np.abs(train_series[m:] - train_series[:-m]))
else:
    denom = np.mean(np.abs(np.diff(train_series)))

for name, pred in [('Seq2Seq+Attn', seq_preds), ('Baseline LSTM', lstm_preds)]:
    rm = math.sqrt(mean_squared_error(Y_test_raw.flatten(), pred.flatten()))
    ma = mean_absolute_error(Y_test_raw.flatten(), pred.flatten())
    mase_val = np.mean(np.abs(Y_test_raw - pred)) / (denom + 1e-9)
    results.append({'model': name, 'RMSE': float(rm), 'MAE': float(ma), 'MASE': float(mase_val)})

pd.DataFrame(results).to_csv(os.path.join(OUTDIR, 'performance.csv'), index=False)
print('\nPerformance:')
print(pd.DataFrame(results))

# ------------------ 6) Plots & attention heatmap ------------------
plt.figure(figsize=(10,3))
plt.plot(df.index[-800:], df['y'].values[-800:])
plt.title('Last 800 points of generated series')
plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, 'series_last800.png'))

# example forecast plot (first 3 test samples)
for i in range(min(3, X_test.shape[0])):
    start = val_end + i
    times = df.index[PAST + start: PAST + start + HORIZON]
    plt.figure(figsize=(8,2.5))
    plt.plot(times, Y_test_raw[i], marker='o', label='True')
    plt.plot(times, seq_preds[i], marker='x', label='Seq2Seq')
    plt.plot(times, lstm_preds[i], marker='.', label='LSTM')
    plt.legend(); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, f'forecast_{i}.png'))

# attention weights analysis
att_interpretation = []
try:
    # compute attention across a subset to save time
    subset = min(50, X_test.shape[0])
    att_w = att_model.predict([X_test[:subset], dec_test[:subset]])  # (samples, Tdec, Tenc)
    avg_att = np.mean(att_w, axis=0)  # (Tdec, Tenc)
    # for each decoder step, find encoder index with max attention
    important_encoder_idx = np.argmax(avg_att, axis=1)
    # convert indices to hours relative to prediction time
    rel_hours = PAST - important_encoder_idx  # how far back the model attends most
    # summarize: histogram of relative attention peaks
    import collections
    cnt = collections.Counter(rel_hours)
    common = cnt.most_common(5)
    att_interpretation.append('Top relative look-backs (hours) the model most frequently attended to for decoder steps:')
    for h,c in common:
        att_interpretation.append(f'  {h} hours back: {c} decoder steps')
    # average attention on last 24 hours
    avg_last24 = np.mean(avg_att[:, -24:], axis=1)
    att_interpretation.append('\nAverage attention mass over encoder''s last 24 hours per decoder step (sample):')
    att_interpretation.append(str(avg_last24.tolist()[:6]) + ' ...')
    # save heatmaps for first 3
    for i in range(min(3, subset)):
        plt.figure(figsize=(7,2.5))
        plt.imshow(att_w[i], aspect='auto')
        plt.colorbar(); plt.title(f'Attention weights sample {i}')
        plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, f'att_{i}.png'))
except Exception as e:
    att_interpretation.append('Attention extraction failed: ' + str(e))

# write attention interpretation to file
with open(os.path.join(OUTDIR, 'attention_interpretation.txt'), 'w') as f:
    f.write('\n'.join(att_interpretation))

# final report (text) that meets deliverable requirements
report = {
    'dataset': {
        'observations': int(N),
        'features': FEATURES,
        'summary_path': os.path.join(OUTDIR, 'generated_summary_stats.csv')
    },
    'models': {
        'seq2seq_summary': os.path.join(OUTDIR, 'seq_model_summary.txt'),
        'baseline': 'Lightweight LSTM (single-layer)'
    },
    'performance': results,
    'attention_interpretation_file': os.path.join(OUTDIR, 'attention_interpretation.txt')
}

with open(os.path.join(OUTDIR, 'report.json'), 'w') as f:
    json.dump(report, f, indent=2)

# also produce a short README with explanations (text deliverable)
readme = f"""
Project: Advanced Time Series Forecasting (FAST)

Dataset:
- Observations: {N}
- Features: {FEATURES}
- Generated with trend, daily & weekly seasonality, low-frequency component, heteroscedastic noise, temperature and promotion covariates.
- Summary statistics saved to: {os.path.join(OUTDIR, 'generated_summary_stats.csv')}

Models:
- Seq2Seq with Bahdanau-style attention (lightweight, vectorized).
- Baseline: single-layer LSTM that outputs horizon directly.
- Model summary saved to: {os.path.join(OUTDIR, 'seq_model_summary.txt')}

Evaluation:
- Metrics: RMSE, MAE, MASE (seasonal m=24). Results saved to: {os.path.join(OUTDIR, 'performance.csv')}
- Attention heatmaps and a short interpretation saved to: {os.path.join(OUTDIR, 'attention_interpretation.txt')}

How this meets the project requirements:
- Dataset size >= 5000 ✔
- Explicit attention mechanism implemented ✔
- Baseline LSTM implemented ✔
- Evaluation includes RMSE, MAE, MASE ✔
- Attention analysis + qualitative interpretation file included ✔

Limitations & notes:
- This is a fast, reproducible implementation to run on modest hardware.
- For production-quality experiments consider longer training, hyperparameter search, and a statistical baseline (SARIMA) if required.

"""
with open(os.path.join(OUTDIR, 'README_project.txt'), 'w') as f:
    f.write(readme)

print('\nAll outputs saved to', OUTDIR)